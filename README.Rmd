---
output: github_document
always_allow_html: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-"
)
```

This package contains various workflow functions for working with data within the Sacramento-San Joaquin River Delta. There is a primary focus on IEP (Interagency Ecological Program) Surveys and supporting their data publication workflow. Additional features pertaining to other surveys may be supported in the future.

```{r setup}
library(deltadata)
```

## Installation

The `deltadata` package can be installed from its GitHub repository.
```{r, eval=F}
# Install `devtools` if we need to
install.packages("devtools")

# Installing `deltadata` if we do not already have it
devtools::install_github("trinhxuann/deltadata")
```

## Reading in the data

The `bridgeAccess()` function can be used to connect to an Access database and extract any table of interest. First and foremost, we will need a 32-bit version of R for this function to work due to the limitations of the odbc drivers. The newest version of R that supports 32-bit is [4.1.3](https://cran.r-project.org/bin/windows/base/old/4.1.3)--32-bit R is no longer supported after this version. Once that is satisfied, we can work in 64-bit R (32-bit R will be invoked when appropriate).

The function can connect directly to a URL source or to a file on our hard drive. When connecting to a URL, it is suggested to download from a zipped source to minimize the file size--the function will extract the Access file from the zipped file. If we then only provide the file path in the function, a lost of available tables will be provided to us to explore. The function can also access system tables, however, we will be prompted to provide file permissions before the function can pull those tables. As a case study, we will work with the SLS Survey:

First, download the database (zipped form) and explore what tables are available to extract--the database file will only be downloaded once as long as we remain in the same R session (downloaded to the temporary folder):
```{r}
bridgeAccess("https://filelib.wildlife.ca.gov/Public/Delta%20Smelt/SLS.zip")
```

SLS has 7 main relational tables of interest:
```{r}
slsTables <- bridgeAccess("https://filelib.wildlife.ca.gov/Public/Delta%20Smelt/SLS.zip",
                          tables = c("Catch", "FishCodes", "Lengths", "Meter Corrections",
                                     "SLS Stations", "Tow Info", "Water Info"))
```

We can also download the relationship table. This is an important table that records the relationships between the relational tables. Unfortunately, this table does require special permissions; but fortunately, we can easily give ourselves the necessary permission. When we first try to pull the table, an error is given and the Access file will be opened. The function will then output in the console's error message instructions on how to give ourselves permissions: in the program, "Enable content, `Ctrl + g`, enter `CurrentProject.Connection.Execute "GRANT SELECT ON MSysRelationships TO Admin;"`, `Enter`, exit file, and rerun this code."

```{r}
schema <- bridgeAccess(
  "https://filelib.wildlife.ca.gov/Public/Delta%20Smelt/SLS.zip",
  tables = c("MSysRelationships"), retry = T
  )

# The function outputs a list, of which we can index to grab just the table
schema <- schema[[1]]
```
## Data joining

The schema table contains instructions on how to join our relational tables together. The `schemaJoin()` function deciphers this table and does the joining for us. This approach is theoretically preferable compared to joining by hand as it removes any guesswork on how we should join the tables together--this is not absolute though, since it depends on the database being set up correctly in the first place.

```{r}
joinedData <- schemaJoin(schema, slsTables)
```
Throughout the joining process, the function provides a narration of the join type, the two relational tables being joined, and the column keys the join is occurring on. We can use this narration to ensure that we are getting the joins we are expecting. If these are not the joins that we want, the function can take in any schema table, i.e., we can modify what join type(s) we would like the function to use by specifying the `joinType` column in the schema table. The four join options are: `full_join`, `inner_join`, `left_join`, and `right_join`.

```{r}
schemaFullJoin <- schema
schemaFullJoin$joinType <- "full_join"
joinedDataFullJoin <- schemaJoin(schemaFullJoin, slsTables)

# Would expect number of rows to differ between an inner vs full join
data.frame(joinType = c("auto", "fullJoin"),
           nRows = c(nrow(joinedData), nrow(joinedDataFullJoin)),
           nCols = c(ncol(joinedData), ncol(joinedDataFullJoin)))
```

### Aside on `schemaJoin()`

Besides supplying our own join type to the schema table, we can also provide manipulated relational tables to the function, e.g., tables in `slsTables` in the example above. There are several reasons why this may be required: 1) we want to be sure our columns are read in correctly, e.g., date data as a Date format, 2) column names need to change, 3) unit conversion, or 4) other reasons. Ultimately, this function simply looks at the schema table and applies those joins to the provided data--we can manipulate either argument as we like.

## Checking GPS coordinate outliers

Two functions are dedicated to checking outlying GPS coordinates:

1. `plotGPS()`: plots sampling GPS coordinates on a leaflet map. Requires a data frame with 6 columns, `date`, `station`, `legend`, `layer`, `lat`, and `lon.` The `legend` column determines the color labels for the plotted points, and it is recommended that one group be the theoretical coordinates of each sampling station. The `layer` column determines the layer control of the output map and is generally the survey number. See `?plotGPS()` for more information.
2. `gpsOutlier()`: returns a data frame of GPS coordinates that are beyond a specified distance (default is 0.5 mile, measured "as-the-crow-flies", or the most direct path, that takes into account the curvature of the Earth) from each point's theoretical coordinates. This function also requires a data frame with 6 columns, the same as `plotGPS()`, with a firm requirement for a `Theoretical` (named as so) group of GPS coordinates labeled in `legend` column.

```{r}
gpsDF <- slsTables[["Water Info"]]
gpsDF$SeasonYear <- as.numeric(format(gpsDF$Date, format = "%Y")) + 
  (as.numeric(format(gpsDF$Date, format = "%m")) > 11)
gpsDF <- subset(gpsDF, SeasonYear == 2023)

# Split lat/lon into degrees
for (i in c("StartLat", "StartLong", "EndLat", "EndLong")) {
  gpsDF[[i]] <- gsub("[-. ]", "", gpsDF[[i]])
}

# Creating a function to do this since we'll be reusing it later:
createGPS <- function(data) {

  data$StartLatD <- as.numeric(substr(data$StartLat, 1, 2))
  data$StartLatM <- as.numeric(substr(data$StartLat, 3, 4))
  data$StartLatS <- as.numeric(substr(data$StartLat, 5, 7))
  data$EndLatD <- as.numeric(substr(data$EndLat, 1, 2))
  data$EndLatM <- as.numeric(substr(data$EndLat, 3, 4))
  data$EndLatS <- as.numeric(substr(data$EndLat, 5, 7))
  data$StartLonD <- as.numeric(substr(data$StartLong, 1, 3))
  data$StartLonM <- as.numeric(substr(data$StartLong, 4, 5))
  data$StartLonS <- as.numeric(substr(data$StartLong, 6, 8))
  data$EndLonD <- as.numeric(substr(data$EndLong, 1, 3))
  data$EndLonM <- as.numeric(substr(data$EndLong, 4, 5))
  data$EndLonS <- as.numeric(substr(data$EndLong, 6, 8))
  # Note the seconds calculation here. 36000 due to format of the LatS columns
  data$Start_lat <- data$StartLatD + data$StartLatM/60 + data$StartLatS/36000
  data$End_lat <- data$EndLatD + data$EndLatM/60 + data$EndLatS/36000
  data$Start_lon <- -(data$StartLonD + data$StartLonM/60 + data$StartLonS/36000)
  data$End_lon <- -(data$EndLonD + data$EndLonM/60 + data$EndLonS/36000)

  data
}

gpsDF <- createGPS(gpsDF)
# Required columns for plotGPS()
gpsDF$date <- gpsDF$SeasonYear
gpsDF$layer <- gpsDF$Survey

# GPS start coordinates
gpsLat<- gpsDF[, which(!names(gpsDF) %in% c("Start_lon", "End_lon"))]
gpsLat <- reshape(gpsLat, direction = "long",
                        varying = c("Start_lat", "End_lat"),
                        v.names = "lat",
                        times = c("Start", "End"),
                        timevar = "legend")

gpsLon<- gpsDF[, which(!names(gpsDF) %in% c("Start_lat", "End_lat"))]
gpsLon <- reshape(gpsLon, direction = "long",
                  varying = c("Start_lon", "End_lon"),
                  v.names = "lon",
                  times = c("Start", "End"),
                  timevar = "legend")

gpsDFLong <- cbind(gpsLat, lon = gpsLon$lon)

# Joining in the theoretical coordinates
gpsTheoretical <- slsTables$`SLS Stations`

for (i in c("LatD", "LatM", "LatS", "LonD", "LonM", "LonS")) {
  gpsTheoretical[[i]] <- as.numeric(gpsTheoretical[[i]])
}

gpsTheoretical$lat <- gpsTheoretical$LatD + gpsTheoretical$LatM/60 + 
  gpsTheoretical$LatS/3600
gpsTheoretical$lon <- -(gpsTheoretical$LonD + gpsTheoretical$LonM/60 + 
                          gpsTheoretical$LonS/3600)
gpsTheoretical$legend <- "Theoretical"
gpsTheoretical$layer <- "Theoretical"
gpsTheoretical$date <- NA

gpsData <- rbind(gpsDFLong[, c("date", "Station", "legend", "layer", "lat", 
                               "lon")],
                 gpsTheoretical[, c("date", "Station", "legend", "layer", "lat", 
                                    "lon")])

# Can leave the NAs in, but plotGPS will throw back warnings about them
gpsData <- subset(gpsData, !is.na(lat) | !is.na(lon))

# layerName and dateName are specific to the popup labels at each point.
plotGPS(gpsData, layerName = "Survey", dateName = "Year", height = 500)
```

`plotGPS()` allows us to quickly visualize our GPS coordinates. It is most useful when we also visualize the theoretical coordinates alongside our points of interest. The outputted leaflet map has several useful features: 1) an interactive map in which we can zoom in and out and move around, 2) a legend if we have multiple groupings of the same sampling location, 3) layer controls that allow us to see only the layers we want, and 4) labeled GPS point that also provides additional data when clicked upon. The function allows us to quickly see the most egregious outliers.

We can also do a finer search for outliers using `gpsOutlier()`. This function calculates the distance between each sampling point and that location's theoretical GPS coordinates and returns any points that exceeds the distance threshold (defaults to 0.5 mile). This implies that we must supply the theoretical GPS coordinates of our sampling points to the function and is why it is recommended to construct our GPS data frame to include the theoretical when using `plotGPS()`. The data frame output for `gpsOutlier()` is the same format as required by `plotGPS()`, allowing us to pipe the two together.

```{r}
# By default, d = 0.5
gpsOutliersData <- gpsOutlier(gpsData)

plotGPS(gpsOutliersData, height = 500)

head(gpsOutliersData)
```

The outputted data frame from `gpsOutlier()` will feature an `outlier` column alongside the distance from the theoretical point as `distance`. The default distance threshold of 0.5 mile is the ideal target for most CDFW survey trawls.

## Comparing water quality values to CDEC gages

`popCDEC()` is used to populate a data frame of sampling points with water temperature, water turbidity, or water electroconductivity data from the nearest CDEC gage. Only these metrics are currently supported. The input data frame requires at least five columns: 1) `station` (name of the sampling point), 2) `lat` (latitude of the sampling point), 3) `lon` (longitude of the sampling point), 4) `time` (date-time of the sample), and 5) the water quality variable of interest (`temp`, `ec`, or `turbidity`).

```{r}
# Using the joined SLS data as an example
waterQualityData <- subset(
  joinedData, format(Date, format = "%Y") == "2023" &
    Survey == 1 & (!is.na(StartLat) | !is.na(StartLong))
  )

# Split lat/lon into degrees
for (i in c("StartLat", "StartLong", "EndLat", "EndLong")) {
  waterQualityData[[i]] <- gsub("[-. ]", "", waterQualityData[[i]])
}

# createGPS is a helper function defined on line 108 above.
waterQualityData <- createGPS(waterQualityData)
waterQualityData$time <- paste(
  waterQualityData$Date, 
  format(waterQualityData$Time, format = "%H:%M:%S")
  )
waterQualityData <- unique(waterQualityData[, c("Station", "Start_lat", 
                                                "Start_lon", "time",
                                                "TopTemp", "TopEC", "NTU")])
waterQualityData <- unique(waterQualityData[, names(waterQualityData)])
names(waterQualityData) <- c("Station", "lat", "lon", "time", 
                             "temp", "ec", "turbidity")

startTime <- Sys.time()
cdecTable <- popCDEC(waterQualityData)
totalTime <- Sys.time() - startTime

head(cdecTable)
```

For each sampling point, the function will return the desired water quality data from the nearest CDEC station for each sampling point, in terms of space (as-the-crow-flies) and time. The function also reports back various metadata: the nearest CDEC station (`cdecStation`), the time difference in minutes between the sample of interest and the CDEC sample (`timeDifference`), the distance in miles between the sampling point and the CDEC station (`distance`), and other metadata information about the CDEC station (`sensorNumber`, `sensorDescription`, `units`, `duration`, `dataAvailable`).

### Notes on the nuanced operations by `popCDEC()`

There are several underlying operations of the function to be aware of:

1. the nearest CDEC station is calculated as a as-the-crow-flies distance
2. the reported value from the CDEC station is the closest time point to the requested sampling point that has data. This means that all `NA` data from the CDEC station is ignored, even sampling points that are closer in time to our sample. The reported value is the closest *valid* value in time to when our sampling occurred.
3. the function scrapes from the CDEC website and can get bogged down as we increase the number of data points to populate and/or the range of dates of the initial dataset. It is also dependent on the internet connection speed to the CDEC website
4. if multiple sampling points of interest is provided, the absolute date range of the dataset will be used to download data from each CDEC station of interest. This is very nuanced and can be safely ignored in most instances.

### `pullCDEC`

Function `popCDEC` relies on `pullCDEC()` in the background to pull data from CDEC gages. This function can be used as a stand-alone function. 

```{r}
malGage <- pullCDEC(station = "MAL", sensor = 25, duration = "hourly", 
                    dateStart = "06/13/1986", dateEnd = "06/14/1986")

head(malGage)
```

The function will provide the requested data directly into R if all the arguments are provided. Since it is difficult to know what data is available for a gage of interest, the function also provide metadata for the station if only station argument is specified, e.g., `pullCDEC(station = "MAL")`.

## Conclusion

The `deltadata` package aims to provide workflow functions to efficiently work with IEP survey data. In this vignette, we explored how to use this package to replicate portions of the SLS QAQC pipeline. Function `bridgeAccess` is a convenient wrapper to connect to an Access database, allowing us to pull out relational tables of interest. The function can also access the relationship schema, which can be used to guide the joining process of our relational tables of interest via `schemaJoin()`. Once these relational tables are read into R, we can leverage R package to QAQC our data. The `deltadata` specifically provides several QAQC functions: 1) `plotGPS()` and `gpsOutlier()` to explore outlying GPS coordinates based on the distance from the theoretical, and 2) compare water quality data to the nearest CDEC station via `popCDEC()`.

There may be additional functionalities added to this package in the future. If you have any suggestions or encounter any bugs, please feel free to open an [issue](https://github.com/trinhxuann/deltadata/issues), contribute to the package via a pull request, or directly contact [Trinh Nguyen](mailto:trinh.nguyen@wildlife.ca.gov).
